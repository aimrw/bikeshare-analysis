---
title: "ReadingInData"
output: pdf_document
date: "2023-02-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plyr)
library(dplyr)
library(Rfast)
library(stats)
library(permuco)
```

```{r}
# Load the previously generated data
load("./bikedata.RData") 
```

```{r}
# Create dataframe of all the data
bike_df = as.data.frame(starttime)
bike_df$duration = duration
bike_df$station_start = station_start
bike_df$station_end = station_end
bike_df$bike_num = bikenum
bike_df$member = member
bike_df$days_since_Jan1_2010 = days_since_Jan1_2010
bike_df$day_of_week = day_of_week

convert_to_route = function(x) {
  paste("r",as.character(x),sep="")
}

bike_df$route = paste(lapply(station_end,convert_to_route),lapply(station_start,as.character), sep="_")
bike_df$is_weekday = (bike_df$day_of_week != "Saturday") & (bike_df$day_of_week != "Sunday")

names(bike_df)[names(bike_df) == 'V1'] <- 'Year'
names(bike_df)[names(bike_df) == 'V2'] <- 'Month'
names(bike_df)[names(bike_df) == 'V3'] <- 'Day'
names(bike_df)[names(bike_df) == 'V4'] <- 'Hour'
names(bike_df)[names(bike_df) == 'V5'] <- 'Minute'
names(bike_df)[names(bike_df) == 'V6'] <- 'Second'

# Identify the trips that occurred during rush hour
# Let's define rush hour as 7-9 am and 5-7 pm
# This corresponds to hours 7-9 and 17-19
bike_df$rush_hour = bike_df$Hour %in% c(7,8,9,17,18,19)

#Season
bike_df$Month = as.factor(bike_df$Month)
bike_df$season = revalue(bike_df$Month, c("1"="Winter", "2"="Winter","3"="Spring","4"="Spring","5"="Spring","6"="Summer","7"="Summer","8"="Summer","9"="Fall","10"="Fall","11"="Fall","12"="Winter"))
```

```{r}
# Multiple Testing Functions

# Regular BH procedure
regular_BH = function(p, alpha) {
  n <- length(p)
  k <- 1:n
  idx = order(p)
  reject <- ifelse(p[idx] <= alpha * (k / n), TRUE, FALSE)
  k <- max(which(reject == TRUE))
  if (k == -Inf) {
    return("No signals")
  } else {
    reject[1:k] <- TRUE
    reject = reject[order(p)]
    return(reject)
  }
}

# Storey BH procedure
storey_BH = function(P, alpha, gamma){
    n = length(P)
    order_index = order(P, decreasing=FALSE)
    pi_hat = sum(P>gamma)/n/(1-gamma)
    P[P>gamma] = Inf   # never reject any p > gamma
    ks = which(P[order_index] <= (1:n)*alpha/n/pi_hat)
    if(length(ks)==0){
        return(NULL)
    }else{
        k = max(ks)
        rej_index = order_index[1:k]
        return(rej_index)
    }
}

# Group adaptive BH with groups being the starting station.
group_adaptive_BH_startingstation <- function(P,groups,alpha,gamma) {
  n = length(P)
  pval = split(p_values, groups)
  pi = 1:length(groups)
  for (i in 1:length(pval)) {
    pi[groups == names(pval)[i]] = length(pval[[i]][pval[[i]] > gamma]) / (length(pval[[i]])*(1-gamma))
  }
  p_tilda <- pi * P
  reject = storey_BH(p_tilda, alpha, gamma)
  #p_tilda[P > gamma] <- P[P > gamma] + Inf
  #k <- 1:n
  #reject <- ifelse(sort(p_tilda) <= alpha * (k / n), TRUE, FALSE)
  #k <- max(which(reject == TRUE))
  #reject[1:k] <- TRUE
  #reject = reject[order(p_tilda)]
  return(reject)
}

# Takes in a vector of p-values, alpha, and group size (number of p-values for each route). Runs BH. If a route contains 1 or more rejections, then we consider that route rejected (significant)
confounder_adjustment = function(P, alpha, confounder_group_size) {
  P[is.na(P)] = 1
  l = length(P) / confounder_group_size
  total_rejections = (p.adjust(p_values,method="BH") < 0.05)
  split_rejections = split(total_rejections, rep(1:l, each=confounder_group_size))
  route_rejections = rep(FALSE, l)
  for (i in 1:l) {
    if (TRUE %in% split_rejections[[i]]) {
        route_rejections[i] = TRUE
    }
    else {
        route_rejections[i] = FALSE
    }
  }
  return(route_rejections)
}
```

To determine changes in each route, we started by conducting a permutation test for each route. First, we selected only routes with over 100 data points. We do not think we would get any reliable changes in the routes with too few data points. We think it is also more computationally feasible to run. If we only choose routes that have more than 100 rides, it will cover 86.7% of the original data points.

The test statistic we used is the correlation between the days since Jan 2010 and the duration of the bike ride. We first try this without any confounders and only look at the relationship between the days and the duration. We run 10000 permutations and calculate a p-value for each route.

```{r}
# Get p-values for each route without confounders and run 
permutation_test = function(route_subset, tfunc) {
  b_data = tfunc(route_subset$duration,route_subset$days_since_Jan1_2010)
  unshuffled_routes = matrix(rep(route_subset$days_since_Jan1_2010,10000), ncol=10000)
  shuffled_routes = colShuffle(unshuffled_routes)
  statistics = tfunc(route_subset$duration, shuffled_routes)
  return((1+sum(abs(statistics)>=abs(b_data)))/(1+length(statistics)))
}
```

```{r}
route_count = as.data.frame(table(bike_df$route))
usable = unique(as.character(route_count$Var1[route_count$Freq >= 100]))
p_values = rep(0,length(usable))

for (route in usable) {
  route_df = bike_df[bike_df$route == route,]
  b_data = cor(route_df$duration,route_df$days_since_Jan1_2010)
  unshuffled_routes = matrix(rep(route_df$days_since_Jan1_2010,10000), ncol=10000)
  shuffled_routes = colShuffle(unshuffled_routes)
  correlations = cor(route_df$duration, shuffled_routes)
  #p_values = c(p_values, (1+sum(abs(correlations)>=abs(b_data)))/(1+length(correlations)))
  p_values[usable == route] = (1+sum(abs(correlations)>=abs(b_data)))/(1+length(correlations))
}

save(p_values, file = "no_confounder_p_values_100.rda")

sum(p.adjust(p_values,method="holm") < 0.05)
sum(p.adjust(p_values,method="BH") < 0.05)
usable = unique(as.character(route_count$Var1[route_count$Freq > 100]))
p_values_100 = c()
for (route in usable) {
  route_df = bike_df[bike_df$route == route,]
  p_values_100 = c(p_values_100, permutation_test(route_subset = route_df, cor))
}
save(p_values_100, file = "no_confounder_p_values_100.rda")
```

After getting the p-values, we try different multiple testing procedures. If we were doing a real analysis to report significant results, we would select a procedure beforehand, here we try different procedures to see whether they will reject a different number of routes. For Bonferroni and Holm-Bonferroni, we control the FWER at 0.05 and for the BH procedures, we are controlling FDR at 0.05. We try a group adaptive BH procedure which uses the starting station as the groups because it is possible that if there is a change in duration of a route due to construction near a particular starting station, we would see routes with the same start station be affected by the construction. Therefore, we use the group adapative BH method to give more weight to p-values from certain starting stations.

```{r}
# Bonferroni
sum(p_values < (0.05 / length(p_values)))

# Holm-Bonferroni
sum(p.adjust(p_values,method="holm") < 0.05)

# BH
sum(p.adjust(p_values,method="BH") < 0.05)

# Storey-BH
length(storey_BH(p_values, 0.05, 0.5))

# Group Adaptive BH
groups = gsub('_.{5}', '', usable)
length(group_adaptive_BH_startingstation(p_values, groups, 0.05, 0.5))
```

We observe that methods for controlling FWER do not have any rejections. This may be because the methods are too conservative and we are limited by the number of simulations we are able to run. On the other hand, we do get rejections when using BH. Storey-BH and Group Adaptive BH improve on this even further. 

```{r}
# Now we plot the routes which the group adaptive BH procedure rejects
route_rejected = usable[group_adaptive_BH_startingstation(p_values, groups, 0.05, 0.5)]
for (route in route_rejected) {
    route_df = bike_df[bike_df$route == route,]
    plot(route_df$days_since_Jan1_2010, route_df$duration, 
         xlab="days since Jan1 2020", ylab="duration")
}

```

Having determined the potential confounders, we now try to run the permutation tests accounting for the confounders by permuting within confounder groups. First we try membership status, weekday/weekend, and season as confounders. This gives us 16 combinations of confounders, so we we have 16 confounder groups. Because we need multiple data points to permute within confounder group, it is a further justification for only using route with over 100 data points.

```{r}
# With Membership Status, Weekday/Weekend , Season as confounders
route_count = as.data.frame(table(bike_df$route))
usable = unique(as.character(route_count$Var1[route_count$Freq > 1000]))
p_values_100_confound = c()
for (route in usable) {
  for (membership_status in c(TRUE,FALSE)) {
    for (weekday in c(TRUE,FALSE)) {
      for (in_season in c("Winter","Spring","Summer","Fall")) {
          route_df = bike_df[bike_df$route == route,]
          p_values_1000_confound = c(p_values_100_confound, permutation_test(route_subset = route_df, tfunc = cor))
      }
    }
  }
}
print(p_values_1000_confound[1:100])
save(p_values_1000_confound, file = "real_confounder_p_values_1000.rda")

# Reject a route if any groups within a route are rejected using BH
sum(confounder_adjustment(p_values, 0.05, 16))
```

Then we try rush hour as a confounder instead.

```{r}
# With rush hour instead of season as a confounder 
p_values = c()

route_count = as.data.frame(table(bike_df$route))
usable = unique(as.character(route_count$Var1[route_count$Freq > 1000]))

for (route in usable) {
  for (membership_status in c(TRUE,FALSE)) {
    for (weekday in c(TRUE,FALSE)) {
      for (rush_hour in c(TRUE,FALSE)) {
        route_df = bike_df[(bike_df$route == route) & 
                           (bike_df$member == membership_status) & 
                           (bike_df$is_weekday == weekday) & 
                           (bike_df$rush_hour == rush_hour),]
        b_data = cor(route_df$duration,route_df$days_since_Jan1_2010)
        unshuffled_routes = matrix(rep(route_df$days_since_Jan1_2010,10000), ncol=10000)
        shuffled_routes = colShuffle(unshuffled_routes)
        correlations = cor(route_df$duration, shuffled_routes)
        p_values = c(p_values, (1+sum(abs(correlations)>=abs(b_data)))/(1+length(correlations)))
      }
    }
  }
}
# Reject a route if any groups within a route are rejected using BH
sum(confounder_adjustment(p_values, 0.05, 8))
save(p_values, file = "rush_hour_real_confounder_p_values_1000.rda")
```

Then we try calculating just 1 p-value per route by permuting within confounder groups but only calculating correlation for the entire route. Then we calculate a p-value for that route.

```{r}
# Calculating 1 p-value by permuting within groups but calculating only 1 correlation. 
# With rush hour instead of season as a confounder 
p_values = c()

route_count = as.data.frame(table(bike_df$route))
usable = unique(as.character(route_count$Var1[route_count$Freq > 1000]))
tester = c("r31007_31009", "r31007_31011")
for (route in usable) {
  # get correlation for route
  route_df = bike_df[bike_df$route == route,]
  b_data = cor(route_df$duration,route_df$days_since_Jan1_2010)
  shuffled_routes = c()
  
  for (membership_status in c(TRUE,FALSE)) {
    for (weekday in c(TRUE,FALSE)) {
      for (rush_hour in c(TRUE,FALSE)) {
        route_conf_df = bike_df[(bike_df$route == route) & 
                           (bike_df$member == membership_status) & 
                           (bike_df$is_weekday == weekday) & 
                           (bike_df$rush_hour == rush_hour),]
        unshuffled_routes = matrix(rep(route_conf_df$days_since_Jan1_2010,10000), ncol=10000)
        shuffled_routes = rbind(shuffled_routes, colShuffle(unshuffled_routes))
      }
    }
  }

  correlations = cor(route_df$duration, shuffled_routes)
  p_values = c(p_values, (1+sum(abs(correlations)>=abs(b_data)))/(1+length(correlations)))
}

save(p_values, file = "single_rush_hour_real_confounder_p_values_1000.rda")
```

Using the above p-values, we try different multiple testing procedures again.

```{r}
# Bonferroni
sum(p_values < (0.05 / length(p_values)))

# Holm-Bonferroni
sum(p.adjust(p_values,method="holm") < 0.05)

# BH
sum(p.adjust(p_values,method="BH") < 0.05)

# Storey-BH
length(storey_BH(p_values, 0.05, 0.5))
```




